---
title: "HW8"
author: "Ben Lehmann"
date: "2023-10-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##Q1
a).
We use the maximum likelihood estimation to find the best values for B.
Estimating the parameters of the model by finding the parameter values that maximize the likelihood

b).
The threshold is 0.5, in which the observations with a predicted probability of the positive class is greater than or equal to 0.5 are classified as positive, while those with predicted probabilities less than 0.5 are classified as negative.It could be chosen based on a evaluation of the model's performance with different threshold values.

c).
False Positives = 32
False Negatives = 25
Total= 40 + 32 + 25 + 121 = 218
= (32+25) / 218  = .2615 = %26.15

A false negative is more concerning because it could lead to someone with the illness not receiving medical attention and treatment, potentially endangering their health or the health of others.

d).
i).
^B0 + ^B1X1 + ^B2X2
0.05(40) + 1(3.5) - 6 = -0.5

^p(-0.5) = (e^-0.5)/(1+e^-0.5) = 0.3775 = 37.75

ii).
0.5 = (e^x)/(1+e^x)
1 = e^x
0 = k
0.05X1 + 1(3.5) + (-6) = 0
X1 = 2.5/0.05
X1 = 50

e).
The LDA will do better on the test set because if the true decision boundary
is linear, the bias of LDA will be near 0, the variance of LDA is smaller
and lower than QDA.

f).
QDA will do better on the training set because it has a higher DOF
which  means fitting the training set with QDA is better.

g).

```{r}
X_1 <- rnorm(16)
X_2 <- rnorm(16)

y_v <- c(rep(0, 8), rep(1, 8))

data_f <- data.frame(X_1, X_2, y_v)

log_mod <- glm(y_v ~ X_1 + X_2, data = data_f, family = "binomial")
summary(log_mod)
```

h).

```{r}
library(MASS)
lda_model =lda(y_v ~ X_1 + X_2, data = data_f)
lda_predict = predict(lda_model, data_f)
summary(lda_model)

```


```{r}
new_qda = qda(y_v ~ X_1 + X_2, data = data_f)
qd_p = predict(new_qda,data_f)
summary(new_qda)
```

##Q2

```{r}
spam = read.csv("~/DS303/spambase.data",header=FALSE)
y_or_n = table(spam$V58)
```


```{r}
prop_spam = y_or_n[2] / sum(y_or_n)
prop_not_spam = y_or_n[1] / sum(y_or_n)

cat("Proportion of spam emails: ", prop_spam, "\n")
cat("Proportion of non-spam emails: ", prop_not_spam, "\n")

```


b).

```{r}
set.seed(8)
train_indices = sample(1:nrow(spam), nrow(spam)/2,replace=FALSE)  
testing = (-train_indices)
train_data = spam[train_indices, ]
test_data = spam[-train_indices, ]
```



```{r}
train_prop <- table(train_data$V58)
test_prop <- table(test_data$V58)
```


```{r}
cat("Proportion of spam emails in train set: ", train_prop[2] / sum(train_prop), "\n")
cat("Proportion of non-spam emails in train set: ", train_prop[1] / sum(train_prop), "\n")

cat("Proportion of spam emails in test set: ", test_prop[2] / sum(test_prop), "\n")
cat("Proportion of non-spam emails in test set: ", test_prop[1] / sum(test_prop), "\n")
```


c).

```{r}
model1 <- glm(V58~., data = spam, subset = train_indices, family='binomial')
```


```{r}
predicted_prob <- predict(model1, spam[testing,], type = "response")
head(predicted_prob,10)
```


d).

```{r}
pred = rep(0,length(testing))
pred[predicted_prob >0.5] = 1
table(pred,spam[testing,]$V58)
```

```{r}
1-mean(pred == spam[testing, ]$V58)
```

e).
If an important email is incorrectly classified as spam, it can lead to important messages being missed by the recipient, causing a severe loss.
You can adjust the threshold in your model to minimize false positives even if it results in more false negatives 
##Q3

a)
```{r}
library(ISLR2)

week = Weekly
```



```{r}
model2 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = week, family = 'binomial')
summary(model2)
```

b).

```{r}
Weekly_prob= predict(model2, type='response')
Weekly_pred =rep("Down", length(Weekly_prob))
Weekly_pred[Weekly_prob > 0.5] = "Up"
table(Weekly_pred, Weekly$Direction)
```

I would set the threshold to 0.5, this is the common default. This confusion matrix explains
that we still have errors in our model based on the false positives and false negatives, this
is because of the threshold.
c).
```{r}
train_vals = (week$Year<2009)
Weekly_9010 = week[!train_vals,]
Weekly_fit = glm(Direction~Lag2, data=week,family="binomial", subset=train_vals)
mod_probs = predict(Weekly_fit, week[!train_vals, ], type = "response")
mod_pred = rep("Down", dim(week[!train_vals, ])[1])
mod_pred[mod_probs > 0.5] = "Up"
table(mod_pred, week[!train_vals, ]$Direction)
```


```{r}
mean(mod_pred == week[!train_vals, ]$Direction)
```


d).

```{r}
library(MASS)

lda_mod = lda(Direction ~ Lag2, data = week, subset = train_vals)
lda_pred = predict(lda_mod, week[!train_vals, ])
table(lda_pred$class, week[!train_vals, ]$Direction)
```

```{r}
mean(lda_pred$class == week[!train_vals, ]$Direction)
```


e).

```{r}
qda_fit = qda(Direction ~ Lag2, data = week, subset = train_vals)
qda_pred = predict(qda_fit, week[!train_vals, ])
table(qda_pred$class, week[!train_vals, ]$Direction)
```

```{r}
mean(qda_pred$class == week[!train_vals, ]$Direction)
```

f).

```{r}
library(e1071)

naive_fit = naiveBayes(Direction~Lag2, data=week, subset=train_vals)
naive_pred = predict(naive_fit,week[!train_vals,])
table(naive_pred, week[!train_vals, ]$Direction)
```



